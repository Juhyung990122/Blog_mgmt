---
title: '[Bigdata]하둡실행 에러정리'
date: 2020-11-23 13:21:25
category: Bigdata_log
thumbnail: { thumbnailSrc }
draft: false
---


## 🌟 빅데이터 처리 오류처리로그
빅데이터처리 하둡과제를 진행하면서 발생한 에러와 해결법을 기록합니다.

### 🎯 Container is running beyond virtual memory limits
에러 전체내용: Container [pid=28920,containerID=container_1389136889967_0001_01_000121]
is running beyond virtual memory limits. Current usage: 1.2 GB of 1 GB
physical memory used; 2.2 GB of 2.1 GB virtual memory used.
Killing container.

**문제 :** 맵작업에서 필요한 메모리의 용량이 limit보다 초과되어 나타나는 에러입니다.
**해결 :** 가상환경(vm)내에서 사용할 가용메모리 크기를 늘려주었습니다. 추가로 yarn-site의
메모리 크기 검사 설정을 false로 변경햐였습니다
참고링크는 여기 https://stackoverflow.com/questions/21005643/container-is-running-beyond-memory-limits

### 🎯 java.net authorization error
**문제 :** 네임노드가 실횅되지 않아서 생기는 에러입니다.
**해결 :** start-dfs.sh실행시 네임노드가 정상적으로 켜지지 않아 생기는 문제이므로
hadoop설치 위치 바로 아래의 sbin으로 이동하여 stop-all.sh을 실행합니다.
하둡이 종료되면 hadoop/etc/sbin 내에 있는 core-site.xml에서
tmp directory정보를 삭제한 뒤 저장하고
hadoop namenode -format을 통해 포맷해준뒤
다시 sbin으로 돌아가 start-all.sh 를 실행해주면 정상적으로 작동합니다. 
jps 명령어를 통해 namenode가 정상작동 되는 걸 확인한 뒤에는 
core-site.xml에 tmp 디렉토리 정보를 다시 추가해줍니다.

### 🎯 Could not get lock /var/lib/dpkg/lock - open(11:Resource temporarily unavailable)
**문제 :** apt-get을 사용하면 발생하는 에러입니다.
**해결 :**해당 파일을 열지못해 생기는 에러이니 에러메세지에 나와있는 /var/lib/dpkg/lock을 삭제해주면 됩니다.

